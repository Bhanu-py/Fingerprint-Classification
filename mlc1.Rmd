---
title: "Multi-label Classification"
author: "Bhanu Angam"
date: "5/24/2022"
output: 
  html_document: 
    toc: yes
    theme: paper
---

```{r Libraries, include=FALSE}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidymodels)
library(skimr)
library(scutr)
library(caret)
```

## Data Pre-processing

```{r loading data}
df1 <- read.csv('data/SFinGe_Default.csv', header = FALSE)
df1$quality <- 'default'

df2 <- read.csv('data/SFinGe_HQNoPert.csv', header = FALSE)
df2$quality <- 'hq'

df3 <- read.csv('data/SFinGe_VQAndPert.csv', header = FALSE)
df3$quality <- 'vq'

tot <- rbind(df1,df2,df3)
tot %>% head()
getwd()
```

### Check for NA
```{r Check for NA, echo=FALSE}
colnames(tot)[ncol(tot)-1] <- 'label'
tot$quality <- as.factor(tot$quality)
tot$label <- as.factor(tot$label)
table(tot$quality, tot$label)
sum(is.na(tot))
```
### Remove Outliers
```{r oulier treatment}
cap <- function(x){
  quantiles <- quantile( x, c(.05, 0.25, 0.75, .95 ) )
  x[ x < quantiles[2] - 1.5*IQR(x) ] <- quantiles[1]
  x[ x > quantiles[3] + 1.5*IQR(x) ] <- quantiles[4]
  x}

tot <- tot %>% mutate_if(is.numeric, cap)

boxplot(tot[, 1:6])

```

### Zero Varinace columns
Finding if columns have Zero Variance that gives NAs while scaling
```{r}
x <- cbind(lapply(tot[,1:(ncol(tot)-2)], FUN = var, na.rm = T))

vardf <- data.frame('col' = rownames(x), 'variation' = unlist(x))
vardf$col[round(vardf$variation,4) == 0.000]
zero_var <- vardf[order(vardf$variation),]
str(tot$V1312)
label <- tot$label
quality <- tot$quality
# remove columns with zero variance
tot  <- tot[,!(round(vardf$variation, 4) == 0.000)]
tot$label <- label
tot$quality <- quality
dim(tot)

boxplot(tot[, 1:6])

tot <- (tot[, sapply(tot, function(x) length(unique(x)) > 3)[1:(ncol(tot)-2)]])
dim(tot)

```
### Class Balancing

```{r}
#-------------------------------balancing class using SMOTE for TRAINSET -------------------- 
plot(table(tot$label), type="h")
```
```{r SMOTE}
prop.table(table(tot$label))
tot_de <- tot[tot$quality == 'default',]
tot_hq <- tot[tot$quality == 'hq',]
tot_vq <- tot[tot$quality == 'vq',]
dim(tot_de)

## SMOTE for class A
smoted_deA <- oversample_smote((tot_de %>% select(-quality)), "A", 'label', 700)
smoted_hqA <- oversample_smote((tot_hq %>% select(-quality)), "A", 'label', 700)
smoted_vqA <- oversample_smote((tot_vq %>% select(-quality)), "A", 'label', 700)

smoted_deA$quality <- 'default'
smoted_hqA$quality <- 'hq'
smoted_vqA$quality <- 'vq'

tot <- rbind(tot, smoted_deA, smoted_hqA, smoted_vqA)

## SMOTE for class T
smoted_deT <- oversample_smote((tot_de %>% select(-quality)), "T", 'label', 700)
smoted_hqT <- oversample_smote((tot_hq %>% select(-quality)), "T", 'label', 700)
smoted_vqT <- oversample_smote((tot_vq %>% select(-quality)), "T", 'label', 700)

smoted_deT$quality <- 'default'
smoted_hqT$quality <- 'hq'
smoted_vqT$quality <- 'vq'

tot <- rbind(tot, smoted_deT, smoted_hqT, smoted_vqT)

plot(prop.table(table(tot$label)))

dim(tot)
sum(is.na(tot))
# tot %>% glimpse()

# saving the pre processed SFinGe data 
saveRDS(tot, 'processed_SFinGe.rds')
tot <- readRDS('processed_SFinGe.rds')
```


## Feature Selection
###### Recursive Feature Elimination
```{r}
# ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(tot[,1:(ncol(tot)-2)], tot$label,  rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```



## Train Test Splits
```{r}
set.seed(2022)
# Split data 70%-30% into training set and test set
tot_split <- as_tibble(tot) %>% 
  mutate_if(is.numeric, scale) %>%
  initial_split(prop = 0.70, strata = label)

# Extract data in each split
tot_train <- training(tot_split)
tot_test <- testing(tot_split) %>% select(-label)
ytest <- testing(tot_split)$label

tot_folds <- vfold_cv(training(tot_split), v = 5, strata = label)
# Print the number of observations in each split
cat("Training cases: ", nrow(tot_train), "\n",
    "Test cases: ", nrow(tot_test), sep = "")

```

## Boost Tree
##### model specification
```{r echo=TRUE}

 # XGBoost model specification
xgboost_model <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 100,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %>%
    set_engine("xgboost")

# grid specification
xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )

xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 4
  )

head(xgboost_grid)

xgboost_wf <- 
  workflows::workflow() %>%
  add_model(xgboost_model) %>% 
  add_formula(label ~ .)

# hyperparameter tuning
library(doParallel)
all_cores <- parallel::detectCores(logical = FALSE)
cls <- makePSOCKcluster(all_cores)
registerDoParallel(cls)
# set.seed(234)
# xgboost_tuned <- tune::tune_grid(
#   object = xgboost_wf,
#   resamples = tot_folds,
#   grid = xgboost_grid,
#   metrics = yardstick::metric_set(roc_auc, accuracy),
#   control = tune::control_grid(verbose = TRUE, save_pred = TRUE)
# )
# saveRDS(xgboost_tuned, '../models/xgboost_tuned.rds')
xgboost_tuned <- readRDS('models/SFinGe/xgboost_tuned.rds')
# plot(xgboost_tuned$.notes)
xgboost_tuned %>%
  collect_metrics(metric='accuracy') %>%
  knitr::kable()
xgboost_tuned %>%
  select_best('accuracy')
xgboost_best_param <- xgboost_tuned %>%
                        select_best('roc_auc')

## fit the model on all the training data
# xgboost_final <- xgboost_model %>%
#                   finalize_model(xgboost_best_param)  %>%
#                   # fit the model on all the training data
#                   fit( formula = label ~ .,data = tot_train)

saveRDS(xgboost_final, 'models/SFinGe/xgboost_final.rds')
xgboost_final <- readRDS('models/SFinGe/xgboost_final.rds')

```

#### Testing model on Test data
```{r echo=TRUE}

pred_df <- bind_cols(
    testing(tot_split)$label,
    predict(xgboost_final, tot_test),
    predict(xgboost_final, tot_test, type = "prob"))

colnames(pred_df) <- c("obs","pred","pred_A","pred_L", "pred_R","pred_T","pred_W")
pred_df

confusionMatrix(pred_df$pred, testing(tot_split)$label)
pred_df %>%
  roc_curve(obs, pred_A:pred_W) %>%
  autoplot()

```


#### Feature Importance
```{r}
library(xgboost)
imp_df = xgb.importance(model=xgboost_final$fit)
head(imp_df$Feature, 10)

xgb.importance(model=xgboost_final$fit) %>% xgb.ggplot.importance(top_n=50, measure=NULL, rel_to_first = FALSE) 
dim(imp_df)
saveRDS(imp_df$Feature, 'models/SFinGe/feature_imp.rds')
```


## KNN nearest neighbours 

Selecting Features from Boost Trees
```{r Features}
nf <- 150
dftrain <- training(tot_split) %>% select(imp_df$Feature[1:nf], label)
dftest <- testing(tot_split) %>% select(imp_df$Feature[1:nf])
ytest <- testing(tot_split)$label
df_folds <- vfold_cv(training(tot_split) %>% select(imp_df$Feature[1:nf], label), v = 5, strata = label)

dim(dftrain)
dim(dftest)
```

### Bayesian Optimization for Hyper parameter Tuning
##### Model Specification
```{r Model Specification}
knn_rec <- recipe(label ~ ., data=tot_train) %>%
  step_scale(all_numeric_predictors)

knn_mod <- 
  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

knn_wflow <- 
  workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(knn_rec)

knn_param <- 
  knn_wflow %>% 
  parameters() %>% 
    update(
    neighbors = neighbors(c(3, 30)),
    weight_func = weight_func(values = c("rectangular", "gaussian", "triangular"))
  )

# ctrl <- control_bayes(verbose = TRUE)
# set.seed(2022)
# knn_search <- tune_bayes(knn_wflow, resamples = df_fold, initial = 5, iter = 20,
#                          param_info = knn_param, control = ctrl)
# saveRDS(knn_search, 'models/SFinGe/knn_tuned.rds')
knn_tuned <- readRDS('models/SFinGe/knn_tuned.rds')

```

```{r}
knn_best_param <- knn_tuned %>% select_best('roc_auc')

knn_final <- knn_mod %>% finalize_model(knn_best_param) %>%
                  # fit the model on all the training data
                  fit( formula = label ~ .,data = dftrain)
knn_final$fit$fitted.values
saveRDS(knn_final, 'models/SFinGe/knn_final.rds')
knn_final <- readRDS('models/SFinGe/knn_final.rds')
```

```{r}
pred_df <- bind_cols(
  ytest,
  predict(knn_final, as.data.frame(tot_test)),
  predict(knn_final, as.data.frame(tot_test), type = "prob"))

colnames(pred_df) <- c("obs","pred","pred_A","pred_L", "pred_R","pred_T","pred_W")

confusionMatrix(pred_df$pred, testing(tot_split)$label)
pred_df %>%
  roc_curve(obs, pred_A:pred_W) %>%
  autoplot()
```


## `Support Vector Machines`

```{r tune_svm}
svm_rec <- 
  recipe(label ~ ., data = tot_train) %>%
  step_normalize(all_numeric_predictors()) 

# Create a model specification
svm_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
                    set_mode("classification") %>%
                    set_engine("kernlab")


# Create a workflow that encapsulates a recipe and a model
svm_wflow <-  workflow() %>% add_recipe(svm_rec) %>% 
  add_model(svm_spec)

# Print out workflow
svm_wflow
```
Now that we have specified what parameter to tune, we'll need to figure out a set of possible values to try out then choose the best.

To do this, we'll create a grid! In this example, we'll work through a regular grid of hyperparameter values, try them out, and see what pair results in the best model performance.

### Grid Search
```{r}
set.seed(2056)

# accuracy, a classification metric
roc_res <- metric_set(roc_auc, accuracy)

# Create regular grid of 6 values for each tuning parameters
start_grid <- svm_wflow %>% 
              grid_regular(levels = 2)

# Print out some parameters in our grid
# svm_grid %>% 
#   slice_head(n = 10)

doParallel::registerDoParallel()
svm_initial <- svm_wflow %>% 
              tune_grid(resamples = tot_folds, 
                        grid = start_grid, 
                        metrics = roc_res)

collect_metrics(svm_initial)

```

### Bootstraps

```{r bootstraps}
set.seed(2056)
# Bootstrap resampling
sf_bs <- bootstraps(sf_train, times = 10)

sf_bs

```

### Model tuning via grid search.
We are ready to tune! Letâ€™s use tune_grid() to fit models at all the different values we chose for each tuned hyperparameter.

```{r grid_search}
doParallel::registerDoParallel(4)

# Model tuning via a grid search
set.seed(2056)
svm_res <- tune_grid(
  object = svm_wflow,
  resamples = sf_bs,
  grid = svm_grid
)
svm_res
```

Now that we have our tuning results, we can extract the performance metrics using collect_metrics()

```{r}
svm_res %>% 
  collect_metrics() %>% 
  slice_head(n = 7)
```

### Visualize tuning metrics
```{r}
svm_res %>% 
  collect_metrics() %>% 
  mutate(rbf_sigma = factor(rbf_sigma)) %>% 
  ggplot(mapping = aes(x = cost, y = mean, color = rbf_sigma)) +
  geom_line(size = 1.5, alpha = 0.7) +
  geom_point(size = 2) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "viridis", begin = .1)
```

