---
title: "NIST Classification"
author: "Bhanu Angam"
date: "5/24/2022"
output: 
  html_document: 
    toc: yes
    theme: paper
---

```{r Libraries, include=FALSE}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidymodels)
library(skimr)
library(scutr)
library(caret)
```

## Data Pre-processing

```{r loading data}

df1 <- read.csv('data/NISTDB4-F.csv', header=FALSE)
df1$quality <- 'F'

df2 <- read.csv('data/NISTDB4-S.csv', header=FALSE)
df2$quality <- 'S'

tot <- rbind(df1,df2)
tot %>% head()
```

### Check for NA
```{r Check for NA, echo=FALSE}
colnames(tot)[ncol(tot)-1] <- 'label'
tot$quality <- as.factor(tot$quality)
tot$label <- as.factor(tot$label)
table(tot$quality, tot$label)

map_chr(tot, typeof) %>% 
  tibble() %>% 
  table()
sum(is.na(tot))
```


#### Missing Values Treatment #####
```{r}
df <- modify(tot[,1:(ncol(tot)-2)], is.na) %>% 
  colSums() %>%
  tibble(names = colnames(tot[,1:(ncol(tot)-2)]),missing_values=.) %>% 
  arrange(-missing_values)

hist(df[df$missing_values != 0,]$missing_values,labels = TRUE, xlim = c(0,3300), breaks = 50)

# Removing columns with 50% of total rows having missing values
names <- modify(tot[,1:(ncol(tot)-2)], is.na) %>% 
  colSums() %>%
  tibble(names = colnames(tot[,1:(ncol(tot)-2)]), missing_values=.) %>% 
  filter(missing_values < 1500) %>% 
  select(1)

tot <- tot[c(names$names, 'quality', 'label')]


## Imputing remaining predictors with less than 1500 missing values
library(naniar)
vis_miss(tot[,846:1019])
tot <- tot %>% 
      mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm = T), x))
sum(is.na(tot))
```


### Remove Outliers
```{r oulier treatment}
cap <- function(x){
  quantiles <- quantile( x, c(.05, 0.25, 0.75, .95 ) )
  x[ x < quantiles[2] - 1.5*IQR(x) ] <- quantiles[1]
  x[ x > quantiles[3] + 1.5*IQR(x) ] <- quantiles[4]
  x}

tot <- tot %>% mutate_if(is.numeric, cap)

boxplot(tot[, 1:6])

```

### Zero Varinace columns
Finding if columns have Zero Variance that gives NAs while scaling
```{r}
##### Finding if columns have Zero Variance that gives NAs while scaling
x <- cbind(lapply(tot[,1:(ncol(tot)-2)], FUN = var, na.rm = T))

vardf <- data.frame('col' = rownames(x), 'variation' = unlist(x))
vardf$col[round(vardf$variation, 5) == 0.0000]
zero_var <- vardf[order(vardf$variation),]
# str(tot$V1312)

# remove columns with zero variance
quality <- tot$quality
label <- tot$label
tot  <- tot[,!(round(vardf$variation, 5) == 0.0000)]
tot$label <- label
tot$quality <- quality
rm(quality)
rm(label)
dim(tot) 

boxplot(tot[, 1:6])

```


### Class Balancing
```{r}
table(tot$label)
prop.table(table(tot$label))
tot_f <- tot[tot$quality == 'F',]
tot_s <- tot[tot$quality == 'S',]
dim(tot_f)
smoted_fT <- oversample_smote((tot_f %>% select(-'quality')), "T", "label", 150)
smoted_sT <- oversample_smote((tot_s %>% select(-'quality')), "T", "label", 150)

smoted_fT$quality = 'F'
smoted_sT$quality = 'S'
table(smoted_fT$label)
table(smoted_sT$label)
tot <- rbind(tot, smoted_fT, smoted_sT)

table(tot$label)
prop.table(table(tot$label))

tot %>%
  head()

## Glimpse first 10 columns
head(tot[, 1:10], n = 14)
dim(tot)
sum(is.na(tot))
# tot %>% glimpse()

# saving the pre processed NIST data 
# saveRDS(tot, 'processed_NIST.rds')
tot1 <- readRDS('preprocessed_NIST.rds')
```


## Feature Selection
###### Recursive Feature Elimination
```{r}
# ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(tot[,1:(ncol(tot)-2)], tot$label,  rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```



## Train Test Splits
```{r}
set.seed(2022)
# Split data 70%-30% into training set and test set
tot_split <- as_tibble(tot) %>% 
  mutate_if(is.numeric, scale) %>%
  initial_split(prop = 0.70, strata = label)

# Extract data in each split
tot_train <- training(tot_split)
tot_test <- testing(tot_split) %>% select(-label)

tot_folds <- vfold_cv(training(tot_split), v = 5, strata = label)
# Print the number of observations in each split
cat("Training cases: ", nrow(tot_train), "\n",
    "Test cases: ", nrow(tot_test), sep = "")

```

## Boost Tree
##### model specification
```{r echo=TRUE}

  # XGBoost model specification
xgboost_model <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 100,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %>%
    set_engine("xgboost")

# grid specification
xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )

xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 4
  )

head(xgboost_grid)

xgboost_wf <- workflow() %>%
  add_model(xgboost_model) %>% 
  add_formula(label ~ .)

# hyperparameter tuning
library(doParallel)
all_cores <- detectCores(logical = FALSE)
cls <- makePSOCKcluster(all_cores)
registerDoParallel(cls)
set.seed(234)
xgboost_tuned <- tune_grid(
  object = xgboost_wf,
  resamples = tot_folds,
  grid = xgboost_grid,
  metrics = metric_set(roc_auc, accuracy),
  control = control_grid(verbose = TRUE, save_pred = TRUE)
)
saveRDS(xgboost_tuned, 'models/NIST/xgboost_tuned.rds')
xgboost_tuned <- readRDS('models/NIST/xgboost_tuned.rds')

xgboost_tuned %>%
  collect_metrics(metric='accuracy') %>%
  knitr::kable()
xgboost_tuned %>%
  select_best('accuracy')

xgboost_best_param <- xgboost_tuned %>%
                        select_best('roc_auc')
## fit the model on all the training data
xgboost_final <- xgboost_model %>%
                  finalize_model(xgboost_best_param)  %>%
                  # fit the model on all the training data
                  fit( formula = label ~ .,data = tot_train)
saveRDS(xgboost_final, 'models/NIST/xgboost_final.rds')
xgboost_final <- readRDS('models/NIST/xgboost_final.rds')

```

#### Testing model on Test data
```{r echo=TRUE}

pred_df <- bind_cols(
    testing(tot_split)$label,
    predict(xgboost_final, tot_test),
    predict(xgboost_final, tot_test, type = "prob"))

colnames(pred_df) <- c("obs","pred","pred_A","pred_L", "pred_R","pred_T","pred_W")
pred_df


confusionMatrix(pred_df$pred, testing(tot_split)$label)
pred_df %>%
  roc_curve(obs, pred_A:pred_W) %>%
  autoplot()

```



#### Feature Importance
```{r}
library(xgboost)
imp_df = xgb.importance(model=xgboost_final$fit)
head(imp_df$Feature, 10)

xgb.importance(model=xgboost_final$fit) %>% xgb.ggplot.importance(top_n=50, measure=NULL, rel_to_first = FALSE) 
dim(imp_df)
saveRDS(imp_df$Feature, 'models/NIST/feature_imp.rds')
imp_df <- readRDS('models/NIST/feature_imp.rds')
```

## KNN nearest neighbours 

Selecting Features from Boost Trees
```{r Features}
nf <- 300
dftrain <- training(tot_split) %>% mutate_if(is.numeric,scale) %>% select(imp_df$Feature[1:nf], label)
dftest <- testing(tot_split) %>% mutate_if(is.numeric,scale) %>% select(imp_df$Feature[1:nf])
ytest <- testing(tot_split)$label
df_folds <- vfold_cv(training(tot_split) %>% mutate_if(is.numeric,scale) %>% select(imp_df$Feature[1:nf], label), v = 5, strata = label)

dim(dftrain)
dim(dftest)
```


### Bayesian Optimization for Hyper parameter Tuning
##### Model Specification
```{r Model Specification}
knn_rec <- recipe(label ~ ., data=tot_train) %>%
  step_scale(all_numeric_predictors)

knn_mod <- nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

knn_wflow <- 
  workflow() %>% 
  add_model(knn_mod) %>% 
  add_formula(label ~ .)

knn_param <- 
  knn_wflow %>% 
  parameters() %>% 
    update(
    neighbors = neighbors(c(3, 50)),
    weight_func = weight_func(values = c("rectangular", "gaussian", "triangular"))
  )


library(doParallel)
all_cores <- detectCores(logical = FALSE)
cls <- makePSOCKcluster(all_cores)
registerDoParallel(cls)
ctrl <- control_bayes(verbose = TRUE)
set.seed(2022)
# Hyper parameter tuning by bayesian optimization
knn_search <- tune_bayes(knn_wflow, resamples = df_folds, initial = 5, iter = 20,
                         param_info = knn_param, control = ctrl)
saveRDS(knn_search, 'models/NIST/knn_tuned.rds')
knn_tuned <- readRDS('models/NIST/knn_tuned.rds')

```

```{r}
knn_best_param <- knn_search %>% select_best('roc_auc')

knn_final <- knn_mod %>% 
              finalize_model(knn_best_param) %>%
              fit( formula = label ~ .,data = dftrain)
                
saveRDS(knn_final, 'models/NIST/knn_final.rds')
knn_final <- readRDS('models/NIST/knn_final.rds')
```

```{r}
pred_df <- bind_cols(
  ytest,
  predict(knn_final, dftest),
  predict(knn_final, dftest, type = "prob"))

# pred_df$.pred <- pmax(pred_df$pred_A, pred_df$pred_L, pred_df$pred_R, pred_df$pred_T, pred_df$pred_W)
colnames(pred_df) <- c("obs", "pred","pred_A","pred_L", "pred_R","pred_T","pred_W")
pred_df
cm_knn_NIST <- confusionMatrix(pred_df$pred, ytest)
saveRDS(cm_knn_NIST, 'results/NIST/cm_knn_NIST.rds')
cm_knn_NIST
pred_df %>%
  roc_curve(obs, pred_A:pred_W) %>%
  autoplot()
```




## `Support Vector Machines`

```{r tune_svm}
svm_rec <- 
  recipe(label ~ ., data = tot_train) %>%
  step_normalize(all_numeric_predictors()) 

# Create a model specification
svm_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
                    set_mode("classification") %>%
                    set_engine("kernlab")


# Create a workflow that encapsulates a recipe and a model
svm_wflow <-  workflow() %>% add_recipe(svm_rec) %>% 
  add_model(svm_spec)

# Print out workflow
svm_wflow
```
Now that we have specified what parameter to tune, we'll need to figure out a set of possible values to try out then choose the best.

To do this, we'll create a grid! In this example, we'll work through a regular grid of hyperparameter values, try them out, and see what pair results in the best model performance.

### Grid Search
```{r}
set.seed(2056)

# accuracy, a classification metric
roc_res <- metric_set(roc_auc, accuracy)

# Create regular grid of 6 values for each tuning parameters
start_grid <- svm_wflow %>% 
              grid_regular(levels = 2)

# Print out some parameters in our grid
# svm_grid %>% 
#   slice_head(n = 10)

doParallel::registerDoParallel()
svm_initial <- svm_wflow %>% 
              tune_grid(resamples = df_folds, 
                        grid = start_grid, 
                        metrics = roc_res)

collect_metrics(svm_initial)

```

### Bootstraps

```{r bootstraps}
set.seed(2056)
# Bootstrap resampling
sf_bs <- bootstraps(sf_train, times = 10)

sf_bs

```

### Model tuning via grid search.
We are ready to tune! Let’s use tune_grid() to fit models at all the different values we chose for each tuned hyperparameter.

```{r grid_search}
doParallel::registerDoParallel(4)

# Model tuning via a grid search
set.seed(2056)
svm_res <- tune_grid(
  object = svm_wflow,
  resamples = sf_bs,
  grid = svm_grid
)
svm_res
```

Now that we have our tuning results, we can extract the performance metrics using collect_metrics()

```{r}
svm_res %>% 
  collect_metrics() %>% 
  slice_head(n = 7)
```

### Visualize tuning metrics
```{r}
svm_res %>% 
  collect_metrics() %>% 
  mutate(rbf_sigma = factor(rbf_sigma)) %>% 
  ggplot(mapping = aes(x = cost, y = mean, color = rbf_sigma)) +
  geom_line(size = 1.5, alpha = 0.7) +
  geom_point(size = 2) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "viridis", begin = .1)
```

